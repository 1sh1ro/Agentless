import csv
import json
import os
import subprocess
import requests
import ast
from clang.cindex import Index, CursorKind

# --- 1. é…ç½®åŒºåŸŸ ---

# è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨æœ¬åœ°çš„ Linux å†…æ ¸ä»£ç ä»“åº“è·¯å¾„
LINUX_REPO_PATH = "/root/Agentless/linux/" 

# è¾“å…¥æ–‡ä»¶
CSV_FILE_PATH = 'merged_output.csv'  # æ‚¨çš„ CSV æ–‡ä»¶å
JSONL_FILE_PATH = '2.jsonl'

# å¤§æ¨¡å‹ API é…ç½® (è¯·å¡«å…¥æ‚¨çš„çœŸå®ä¿¡æ¯)
API_URL = "https://api.deepseek.com/chat/completions"
API_KEY = ""# use yooooooooooooooooooooooooooooour key

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "llm_results"

# æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
MAX_CONTEXT_LENGTH = 60000

# æ–‡ä»¶å†…å®¹æ¨¡æ¿ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
FILE_CONTENT_TEMPLATE = """
### File: {file_name} ###
{file_content}
"""

# --- 2. Prompt æ¨¡æ¿ ---

OBTAIN_RELEVANT_FUNCTIONS_PROMPT = """
Please analyze the following GitHub Problem Description and Structured Code Information.
The code information has been extracted using libclang and shows functions, structures, variables, and macros.

Your task is to identify the Top-10 most relevant code locations that need inspection or editing to fix the problem.
Focus on the actual functions, structures, variables, and macros that are directly related to the issue.

### GitHub Problem Description ###
{problem_statement}

### Structured Code Information ###
{file_contents}

###

Please provide the complete set of locations in this format:
- For functions: function: function_name
- For structures: struct: struct_name  
- For variables: variable: variable_name
- For macros: macro: macro_name

### Examples:
```
fs/ext4/inode.c
function: ext4_write_begin
function: ext4_write_end
struct: ext4_inode_info

net/core/skbuff.c  
function: skb_copy_data
variable: skb_head_cache
macro: SKB_DATA_ALIGN

mm/page_alloc.c
function: alloc_pages
function: free_pages
struct: free_area
```

Return just the locations wrapped with ```. Focus on the most relevant items based on the problem description.
"""


# --- 3. è¾…åŠ©å‡½æ•° ---

def load_dataset_to_dict(jsonl_path):
    """ä¸€æ¬¡æ€§åŠ è½½ JSONL æ•°æ®åˆ°å­—å…¸ä¸­ï¼Œä»¥ ID ä¸ºé”®ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾ã€‚"""
    dataset = {}
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if 'id' in data:
                        dataset[data['id']] = data
                except json.JSONDecodeError:
                    print(f"è­¦å‘Šï¼šè·³è¿‡æ ¼å¼é”™è¯¯çš„ JSON è¡Œ: {line.strip()}")
        print(f"æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®ä» {jsonl_path}")
        return dataset
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {jsonl_path}")
        exit(1)

def checkout_commit(repo_path, commit_hash):
    """åœ¨æŒ‡å®šçš„ä»“åº“è·¯å¾„ä¸­åˆ‡æ¢åˆ°ç‰¹å®šçš„ commitã€‚"""
    if not os.path.isdir(repo_path):
        print(f"é”™è¯¯ï¼šæŒ‡å®šçš„ Linux ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}")
        return False

    print(f"æ­£åœ¨åˆ‡æ¢åˆ° commit: {commit_hash} ...")
    
    # æ£€æŸ¥å¹¶æ¸…ç†Gité”æ–‡ä»¶
    lock_file = os.path.join(repo_path, '.git', 'index.lock')
    if os.path.exists(lock_file):
        print(f"âš ï¸  å‘ç°Gité”æ–‡ä»¶ï¼Œæ­£åœ¨æ¸…ç†: {lock_file}")
        try:
            os.remove(lock_file)
            print(f"âœ… æˆåŠŸåˆ é™¤Gité”æ–‡ä»¶")
        except Exception as e:
            print(f"âŒ åˆ é™¤é”æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    try:
        # å…ˆæ¸…ç†å·¥ä½œåŒºï¼Œé¿å… checkout å¤±è´¥
        print(f"ğŸ”„ é‡ç½®å·¥ä½œåŒºåˆ°HEAD...")
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=repo_path, check=True, capture_output=True, text=True)
        
        # æ¸…ç†æ‰€æœ‰æœªè¢«è¿½è¸ªçš„æ–‡ä»¶å’Œç›®å½•
        print(f"ğŸ§¹ æ¸…ç†æœªè¿½è¸ªçš„æ–‡ä»¶...")
        subprocess.run(['git', 'clean', '-fd'], cwd=repo_path, check=True, capture_output=True, text=True)
        
        # ç¡®ä¿æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„åˆå¹¶æˆ–å˜åŸºæ“ä½œ
        merge_head = os.path.join(repo_path, '.git', 'MERGE_HEAD')
        rebase_apply = os.path.join(repo_path, '.git', 'rebase-apply')
        rebase_merge = os.path.join(repo_path, '.git', 'rebase-merge')
        
        if os.path.exists(merge_head):
            print(f"ğŸ”„ æ£€æµ‹åˆ°æœªå®Œæˆçš„åˆå¹¶ï¼Œæ­£åœ¨ä¸­æ­¢...")
            subprocess.run(['git', 'merge', '--abort'], cwd=repo_path, check=False, capture_output=True, text=True)
            
        if os.path.exists(rebase_apply) or os.path.exists(rebase_merge):
            print(f"ğŸ”„ æ£€æµ‹åˆ°æœªå®Œæˆçš„å˜åŸºï¼Œæ­£åœ¨ä¸­æ­¢...")
            subprocess.run(['git', 'rebase', '--abort'], cwd=repo_path, check=False, capture_output=True, text=True)
        
        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°åˆ‡æ¢åˆ°ç›®æ ‡ commit
        print(f"ğŸ“¦ åˆ‡æ¢åˆ°ç›®æ ‡commit...")
        result = subprocess.run(['git', 'checkout', commit_hash], cwd=repo_path, check=True, capture_output=True, text=True)
        print("âœ… åˆ‡æ¢ commit æˆåŠŸã€‚")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ åˆ‡æ¢åˆ° commit {commit_hash} å¤±è´¥ã€‚")
        print(f"Git å‘½ä»¤è¾“å‡º:\n{e.stderr}")
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ›´æ¿€è¿›çš„æ¸…ç†
        print(f"ğŸ”„ å°è¯•æ›´æ¿€è¿›çš„æ¸…ç†æ–¹æ³•...")
        try:
            # åˆ é™¤å¯èƒ½å­˜åœ¨çš„å…¶ä»–é”æ–‡ä»¶
            lock_files = [
                os.path.join(repo_path, '.git', 'index.lock'),
                os.path.join(repo_path, '.git', 'HEAD.lock'),
                os.path.join(repo_path, '.git', 'config.lock'),
                os.path.join(repo_path, '.git', 'refs', 'heads', 'master.lock'),
                os.path.join(repo_path, '.git', 'refs', 'heads', 'main.lock'),
            ]
            
            for lock_file in lock_files:
                if os.path.exists(lock_file):
                    print(f"ğŸ—‘ï¸  åˆ é™¤é”æ–‡ä»¶: {lock_file}")
                    os.remove(lock_file)
            
            # å†æ¬¡å°è¯•åˆ‡æ¢
            subprocess.run(['git', 'checkout', commit_hash], cwd=repo_path, check=True, capture_output=True, text=True)
            print("âœ… æ¿€è¿›æ¸…ç†ååˆ‡æ¢æˆåŠŸã€‚")
            return True
            
        except Exception as cleanup_error:
            print(f"âŒ æ¿€è¿›æ¸…ç†ä¹Ÿå¤±è´¥: {cleanup_error}")
            return False
            
    except FileNotFoundError:
        print("âŒ 'git' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿ Git å·²å®‰è£…å¹¶ä½äºæ‚¨çš„ PATH ä¸­ã€‚")
        return False


def read_file_content(file_path):
    """ä½¿ç”¨libclangè§£ææ–‡ä»¶å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯"""
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return f"# æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n"

    print(f"ğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
    try:
        # å¯¹äºC/C++æ–‡ä»¶ï¼Œä½¿ç”¨libclangè§£æ
        if file_path.endswith(('.c', '.cpp', '.cc', '.cxx', '.h', '.hpp')):
            print(f"ğŸ”§ è¯†åˆ«ä¸ºC/C++æ–‡ä»¶ï¼Œä½¿ç”¨libclangè§£æ")
            return extract_c_structures(file_path)
        else:
            print(f"ğŸ“„ è¯†åˆ«ä¸ºå…¶ä»–ç±»å‹æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å¼")
            # å¯¹äºå…¶ä»–æ–‡ä»¶ï¼Œè¯»å–å‰1000è¡Œé¿å…è¶…é™
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                if len(lines) > 1000:
                    content = ''.join(lines[:1000]) + "\n... (æ–‡ä»¶è¢«æˆªæ–­ï¼Œä»…æ˜¾ç¤ºå‰1000è¡Œ) ..."
                    print(f"ğŸ“ æ–‡ä»¶è¿‡é•¿({len(lines)}è¡Œ)ï¼Œæˆªæ–­åˆ°1000è¡Œ")
                else:
                    content = ''.join(lines)
                    print(f"ğŸ“„ è¯»å–å®Œæ•´æ–‡ä»¶({len(lines)}è¡Œ)")
            
            return FILE_CONTENT_TEMPLATE.format(
                file_name=file_path,
                file_content=content
            )
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return f"# è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}\n"

def extract_c_structures(file_path):
    """ä½¿ç”¨libclangæå–C/C++æ–‡ä»¶çš„ç»“æ„åŒ–ä¿¡æ¯"""
    print(f"ğŸ“ å¼€å§‹ä½¿ç”¨libclangè§£ææ–‡ä»¶: {file_path}")
    try:
        index = Index.create()
        print(f"ğŸ”§ åˆ›å»ºlibclangç´¢å¼•æˆåŠŸ")
        
        # å°è¯•è§£ææ–‡ä»¶
        translation_unit = index.parse(file_path)
        print(f"ğŸ“– è§£æç¿»è¯‘å•å…ƒæˆåŠŸ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯Šæ–­ä¿¡æ¯ï¼ˆé”™è¯¯/è­¦å‘Šï¼‰
        diagnostics = list(translation_unit.diagnostics)
        if diagnostics:
            print(f"âš ï¸  å‘ç° {len(diagnostics)} ä¸ªè¯Šæ–­ä¿¡æ¯:")
            for diag in diagnostics[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {diag.severity}: {diag.spelling}")
        
        structures = []
        structures.append(f"### File: {file_path} ###")
        
        function_count = 0
        struct_count = 0
        var_count = 0
        macro_count = 0
        
        def visit_node(node, level=0):
            nonlocal function_count, struct_count, var_count, macro_count
            indent = "  " * level
            
            # æå–å‡½æ•°å®šä¹‰
            if node.kind == CursorKind.FUNCTION_DECL:
                func_name = node.spelling
                if func_name and node.is_definition():
                    function_count += 1
                    print(f"ğŸ” å‘ç°å‡½æ•°: {func_name}")
                    # è·å–å‡½æ•°ç­¾å
                    try:
                        start = node.extent.start
                        end = node.extent.end
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            lines = f.readlines()
                            if start.line <= len(lines) and end.line <= len(lines):
                                func_lines = lines[start.line-1:end.line]
                                # åªå–å‡½æ•°å£°æ˜éƒ¨åˆ†ï¼ˆå‰å‡ è¡Œï¼‰
                                func_decl = ''.join(func_lines[:min(3, len(func_lines))])
                                structures.append(f"{indent}FUNCTION: {func_name}")
                                structures.append(f"{indent}  {func_decl.strip()}")
                    except Exception as e:
                        print(f"   âš ï¸ è·å–å‡½æ•° {func_name} çš„æºä»£ç å¤±è´¥: {e}")
                        structures.append(f"{indent}FUNCTION: {func_name}")
            
            # æå–ç»“æ„ä½“/è”åˆä½“å®šä¹‰
         #   elif node.kind in [CursorKind.STRUCT_DECL, CursorKind.UNION_DECL]:
          #      struct_name = node.spelling
           #     if struct_name:
            #        struct_count += 1
             #       print(f"ğŸ—ï¸  å‘ç°ç»“æ„ä½“: {struct_name}")
              #      structures.append(f"{indent}STRUCT: {struct_name}")
                    
            # æå–å…¨å±€å˜é‡
            #elif node.kind == CursorKind.VAR_DECL and level == 0:
             #   var_name = node.spelling
              #  if var_name:
               #     var_count += 1
                #    print(f"ğŸŒ å‘ç°å…¨å±€å˜é‡: {var_name}")
                 #   structures.append(f"{indent}GLOBAL_VAR: {var_name}")
            
            # æå–å®å®šä¹‰
            #elif node.kind == CursorKind.MACRO_DEFINITION:
             #   macro_name = node.spelling
              #  if macro_name:
               #     macro_count += 1
                #    print(f"ğŸ”§ å‘ç°å®: {macro_name}")
                 #   structures.append(f"{indent}MACRO: {macro_name}")
            
            # é€’å½’éå†å­èŠ‚ç‚¹ï¼ˆä½†é™åˆ¶æ·±åº¦ï¼‰
            if level < 3:
                for child in node.get_children():
                    visit_node(child, level + 1)
        
        # å¼€å§‹éå†
        print(f"ğŸ”„ å¼€å§‹éå†ASTèŠ‚ç‚¹...")
        visit_node(translation_unit.cursor)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š è§£æå®Œæˆ! ç»Ÿè®¡ç»“æœ:")
        print(f"   - å‡½æ•°: {function_count} ä¸ª")
        print(f"   - ç»“æ„ä½“: {struct_count} ä¸ª") 
        print(f"   - å…¨å±€å˜é‡: {var_count} ä¸ª")
        print(f"   - å®å®šä¹‰: {macro_count} ä¸ª")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ç»“æ„ä¿¡æ¯ï¼Œå›é€€åˆ°ç®€å•çš„æ–‡æœ¬æ–¹å¼
        # if len(structures) <= 1:
        #     print(f"âš ï¸  æ²¡æœ‰æå–åˆ°ä»»ä½•ç»“æ„åŒ–ä¿¡æ¯ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å¼")
        #     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        #         lines = f.readlines()
        #         content = ''.join(lines[:500])  # åªè¯»å–å‰500è¡Œ
        #         return FILE_CONTENT_TEMPLATE.format(
        #             file_name=file_path,
        #             file_content=content + "\n... (ä½¿ç”¨libclangè§£æå¤±è´¥ï¼Œæ˜¾ç¤ºå‰500è¡Œ)"
        #         )
        
        # print(f"âœ… libclangè§£ææˆåŠŸï¼Œæå–åˆ° {len(structures)-1} é¡¹ç»“æ„åŒ–ä¿¡æ¯")
        return '\n'.join(structures) + '\n'
        
    except Exception as e:
        print(f"âŒ libclangè§£æå¤±è´¥: {e}")
        # libclangè§£æå¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆ
        try:
            print(f"ğŸ”„ å›é€€åˆ°ç®€å•æ–‡æœ¬è¯»å–æ¨¡å¼...")
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                content = ''.join(lines[:500])  # åªè¯»å–å‰500è¡Œ
                print(f"ğŸ“„ æˆåŠŸè¯»å–æ–‡ä»¶å‰500è¡Œï¼Œå…± {len(lines)} è¡Œ")
                return FILE_CONTENT_TEMPLATE.format(
                    file_name=file_path,
                    file_content=content + f"\n... (libclangè§£æå¤±è´¥: {e}ï¼Œæ˜¾ç¤ºå‰500è¡Œ)"
                )
        except Exception as read_error:
            print(f"âŒ æ–‡ä»¶è¯»å–ä¹Ÿå¤±è´¥: {read_error}")
            return f"# è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}\n"

def query_llm(prompt):
    """å‘å¤§æ¨¡å‹å‘é€è¯·æ±‚å¹¶è¿”å›ç»“æœã€‚"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ä½¿ç”¨æ­£ç¡®çš„DeepSeek APIæ ¼å¼
    payload = {
        "model": "deepseek-chat",  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
        "messages": [
            {"role": "system", "content": "You are a helpful assistant specialized in code analysis and debugging."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 1024,
        "temperature": 0.0,
        "stream": False  # æ˜ç¡®æŒ‡å®šä¸ä½¿ç”¨æµå¼è¾“å‡º
    }

    print("æ­£åœ¨å‘å¤§æ¨¡å‹å‘é€è¯·æ±‚...")
    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=120)
        
        # æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
        if response.status_code != 200:
            print(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            return None
            
        response.raise_for_status()

        response_data = response.json()
        # é€‚é…ä¸åŒçš„APIè¿”å›æ ¼å¼
        if "choices" in response_data and response_data["choices"]:
            # æ”¯æŒmessagesæ ¼å¼çš„API
            if "message" in response_data["choices"][0]:
                result_text = response_data["choices"][0]["message"].get("content", "")
            # æ”¯æŒtextæ ¼å¼çš„API
            else:
                result_text = response_data["choices"][0].get("text", "")
            return result_text.strip()
        else:
            print(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {response_data}")
            return f"æœªèƒ½ä» API å“åº”ä¸­æå–æœ‰æ•ˆå†…å®¹: {response.text}"

    except requests.exceptions.RequestException as e:
        print(f"é”™è¯¯ï¼šAPI è¯·æ±‚å¤±è´¥: {e}")
        return None


def num_tokens_from_messages(message, model_name):
    """ä¼°ç®—tokenæ•°é‡çš„ç®€å•å®ç°ï¼ˆåŸå§‹ç‰ˆæœ¬ä½¿ç”¨æ›´å¤æ‚çš„è®¡ç®—ï¼‰"""
    # ç®€å•ä¼°ç®—ï¼šä¸€èˆ¬æ¥è¯´ï¼Œ1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
    return len(message) // 4

def extract_code_blocks(text):
    """ä»LLMè¾“å‡ºä¸­æå–ä»£ç å—ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰"""
    if '```' in text:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª```ä¹‹é—´çš„å†…å®¹
        parts = text.split('```')
        if len(parts) >= 3:
            return parts[1].strip()
    return text.strip()

def extract_locs_for_files(model_found_locs, file_names, keep_old_order=False):
    """è§£æå®šä½ç»“æœå¹¶æŒ‰æ–‡ä»¶åˆ†ç»„ï¼ˆé€‚é…libclangç»“æ„åŒ–è¾“å‡ºï¼‰"""
    result = {}
    lines = model_found_locs.split('\n')
    current_file = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶è·¯å¾„
        if any(line.endswith(ext) for ext in ['.c', '.h', '.py', '.cpp', '.cc', '.cxx', '.hpp']) or '/' in line:
            current_file = line
            if current_file not in result:
                result[current_file] = []
        elif current_file and line.startswith(('function:', 'struct:', 'variable:', 'macro:', 'class:')):
            result[current_file].append(line)
    
    return result

def cleanup_git_locks(repo_path):
    """æ¸…ç†Gitä»“åº“ä¸­çš„æ‰€æœ‰é”æ–‡ä»¶"""
    print(f"ğŸ§¹ å¼€å§‹æ¸…ç†Gité”æ–‡ä»¶...")
    
    lock_patterns = [
        '.git/index.lock',
        '.git/HEAD.lock',
        '.git/config.lock',
        '.git/refs/heads/*.lock',
        '.git/refs/remotes/*/*.lock',
        '.git/objects/pack/*.lock',
    ]
    
    import glob
    cleaned_count = 0
    
    for pattern in lock_patterns:
        full_pattern = os.path.join(repo_path, pattern)
        for lock_file in glob.glob(full_pattern):
            try:
                if os.path.exists(lock_file):
                    os.remove(lock_file)
                    print(f"   âœ… åˆ é™¤: {os.path.relpath(lock_file, repo_path)}")
                    cleaned_count += 1
            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥ {lock_file}: {e}")
    
    if cleaned_count > 0:
        print(f"ğŸ¯ å…±æ¸…ç†äº† {cleaned_count} ä¸ªé”æ–‡ä»¶")
    else:
        print(f"â„¹ï¸  æ²¡æœ‰å‘ç°é”æ–‡ä»¶")
    
    return cleaned_count > 0

# --- 4. ä¸»æ‰§è¡Œé€»è¾‘ ---

def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªæµç¨‹ã€‚"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    # é¢„å…ˆæ¸…ç†Gité”æ–‡ä»¶ï¼Œé¿å…åç»­é—®é¢˜
    print(f"ğŸ”§ é¢„æ£€æŸ¥Gitä»“åº“çŠ¶æ€...")
    if os.path.exists(LINUX_REPO_PATH):
        cleanup_git_locks(LINUX_REPO_PATH)
    else:
        print(f"âš ï¸  Linuxä»“åº“è·¯å¾„ä¸å­˜åœ¨: {LINUX_REPO_PATH}")

    # æ£€æŸ¥ç°æœ‰CSVæ–‡ä»¶çŠ¶æ€
    raw_output_csv = os.path.join(OUTPUT_DIR, "raw_outputs.csv")
    locations_csv = os.path.join(OUTPUT_DIR, "locations.csv")
    
    if os.path.exists(raw_output_csv):
        with open(raw_output_csv, 'r', encoding='utf-8') as f:
            existing_raw_count = sum(1 for line in f) - 1  # å‡å»è¡¨å¤´
        print(f"ğŸ“„ å‘ç°ç°æœ‰åŸå§‹è¾“å‡ºæ–‡ä»¶ï¼Œå·²æœ‰ {existing_raw_count} æ¡è®°å½•")
    else:
        print(f"ğŸ“„ åŸå§‹è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        
    if os.path.exists(locations_csv):
        with open(locations_csv, 'r', encoding='utf-8') as f:
            existing_loc_count = sum(1 for line in f) - 1  # å‡å»è¡¨å¤´
        print(f"ğŸ“„ å‘ç°ç°æœ‰ä½ç½®ä¿¡æ¯æ–‡ä»¶ï¼Œå·²æœ‰ {existing_loc_count} æ¡è®°å½•")
    else:
        print(f"ğŸ“„ ä½ç½®ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")

    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset_to_dict(JSONL_FILE_PATH)
    if not dataset:
        return

    # è¯»å–å¹¶å¤„ç† CSV æ–‡ä»¶
    try:
        with open(CSV_FILE_PATH, mode='r', encoding='utf-8') as csvfile:
            reader = csv.reader(csvfile)
            header = next(reader)  # è·³è¿‡è¡¨å¤´
            
            # å°†æ‰€æœ‰è¡Œè¯»å…¥åˆ—è¡¨
            all_rows = list(reader)
            total_rows = len(all_rows)
            
            # ä»ç¬¬141è¡Œå¼€å§‹å¤„ç†ï¼ˆç´¢å¼•ä»0å¼€å§‹ï¼Œæ‰€ä»¥æ˜¯140ï¼‰
            start_index = 128  # ç¬¬141è¡Œå¯¹åº”ç´¢å¼•140
            
            print(f"å¼€å§‹å¤„ç† CSV æ–‡ä»¶: {CSV_FILE_PATH}")
            print(f"æ€»å…± {total_rows} è¡Œæ•°æ®ï¼Œä»ç¬¬ {start_index + 1} è¡Œå¼€å§‹å¤„ç†")
            print("-" * 50)

            for i in range(start_index, total_rows):
                row = all_rows[i]
                instance_id, files_str = row
                print(f"\nå¤„ç†ç¬¬ {i+1} è¡Œ: instance_id = {instance_id} (è¿›åº¦: {i+1-start_index}/{total_rows-start_index})")

                # ä»æ•°æ®é›†ä¸­æŸ¥æ‰¾å¯¹åº”ä¿¡æ¯
                if instance_id not in dataset:
                    print(f"è­¦å‘Šï¼šåœ¨ {JSONL_FILE_PATH} ä¸­æ‰¾ä¸åˆ° instance_id '{instance_id}'ï¼Œè·³è¿‡æ­¤è¡Œã€‚")
                    continue

                bug_data = dataset[instance_id]
                commit_hash = bug_data.get('commit')
                problem_statement = bug_data.get('report')

                if not commit_hash or not problem_statement:
                    print(f"è­¦å‘Šï¼šinstance_id '{instance_id}' çš„æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡ã€‚")
                    continue

                # 1. åˆ‡æ¢ Git Commitï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                max_retries = 3
                retry_count = 0
                checkout_success = False
                
                while retry_count < max_retries and not checkout_success:
                    if retry_count > 0:
                        print(f"ğŸ”„ ç¬¬ {retry_count + 1} æ¬¡å°è¯•åˆ‡æ¢commit...")
                        # ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                        import time
                        time.sleep(2)
                        # é‡æ–°æ¸…ç†é”æ–‡ä»¶
                        cleanup_git_locks(LINUX_REPO_PATH)
                    
                    checkout_success = checkout_commit(LINUX_REPO_PATH, commit_hash)
                    retry_count += 1
                
                if not checkout_success:
                    print(f"âŒ ç»è¿‡ {max_retries} æ¬¡å°è¯•ä»æ— æ³•åˆ‡æ¢åˆ° commitï¼Œè·³è¿‡ instance_id '{instance_id}'ã€‚")
                    continue

                # 2. å‡†å¤‡æ–‡ä»¶å†…å®¹
                try:
                    # ä½¿ç”¨ ast.literal_eval å®‰å…¨åœ°è§£æå­—ç¬¦ä¸²åˆ—è¡¨
                    file_list = ast.literal_eval(files_str)
                    print(f"ğŸ“‹ éœ€è¦åˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶:")
                    for idx, f in enumerate(file_list, 1):
                        print(f"   {idx}. {f}")
                except (ValueError, SyntaxError):
                    print(f"é”™è¯¯ï¼šæ— æ³•è§£ææ–‡ä»¶åˆ—è¡¨å­—ç¬¦ä¸²: {files_str}")
                    continue

                all_file_contents = []
                print("\nğŸ”„ æ­£åœ¨è¯»å–ç›¸å…³æ–‡ä»¶...")
                for rel_path in file_list:
                    full_path = os.path.join(LINUX_REPO_PATH, rel_path)
                    file_content = read_file_content(full_path)
                    all_file_contents.append(file_content)
                    print()  # ç©ºè¡Œåˆ†éš”æ¯ä¸ªæ–‡ä»¶çš„è¾“å‡º

                print(f"ğŸ“Š æ–‡ä»¶è¯»å–å®Œæˆï¼Œå…±å¤„ç† {len(all_file_contents)} ä¸ªæ–‡ä»¶")

                # 3. æ„å»ºåˆå§‹Promptï¼ˆä½¿ç”¨æ¢è¡Œåˆ†éš”ä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶é•¿åº¦ï¼‰
                file_contents_agg = "\n\n".join(all_file_contents)
                final_prompt = OBTAIN_RELEVANT_FUNCTIONS_PROMPT.format(
                    problem_statement=problem_statement,
                    file_contents=file_contents_agg
                )
                
                initial_token_count = num_tokens_from_messages(final_prompt, "deepseek-chat")
                print(f"ğŸ“ åˆå§‹Prompté•¿åº¦: {initial_token_count} tokens (é™åˆ¶: {MAX_CONTEXT_LENGTH})")

                # 4. Tokené•¿åº¦æ§åˆ¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°ï¼‰
                def message_too_long(message):
                    return num_tokens_from_messages(message, "deepseek-chat") >= MAX_CONTEXT_LENGTH

                reduction_count = 0
                while message_too_long(final_prompt) and len(all_file_contents) > 1:
                    reduction_count += 1
                    print(f"âš ï¸  æ¶ˆæ¯è¿‡é•¿ï¼Œç¬¬{reduction_count}æ¬¡å‡å°‘æ–‡ä»¶æ•°é‡: {len(all_file_contents)} -> {len(all_file_contents)-1}")
                    all_file_contents = all_file_contents[:-1]
                    file_contents_agg = "\n\n".join(all_file_contents)
                    final_prompt = OBTAIN_RELEVANT_FUNCTIONS_PROMPT.format(
                        problem_statement=problem_statement,
                        file_contents=file_contents_agg
                    )
                    current_tokens = num_tokens_from_messages(final_prompt, "deepseek-chat")
                    print(f"   ğŸ“ å½“å‰é•¿åº¦: {current_tokens} tokens")

                if message_too_long(final_prompt):
                    final_tokens = num_tokens_from_messages(final_prompt, "deepseek-chat")
                    print(f"âŒ å³ä½¿å‡å°‘æ–‡ä»¶æ•°é‡ï¼Œæ¶ˆæ¯ä»ç„¶å¤ªé•¿ ({final_tokens} tokens)ï¼Œè·³è¿‡ instance_id '{instance_id}'")
                    continue
                
                if reduction_count > 0:
                    final_tokens = num_tokens_from_messages(final_prompt, "deepseek-chat")
                    print(f"âœ… Tokené•¿åº¦æ§åˆ¶å®Œæˆï¼Œæœ€ç»ˆä½¿ç”¨ {len(all_file_contents)} ä¸ªæ–‡ä»¶ï¼Œ{final_tokens} tokens")
                else:
                    print(f"âœ… Tokené•¿åº¦ç¬¦åˆè¦æ±‚ï¼Œä½¿ç”¨å…¨éƒ¨ {len(all_file_contents)} ä¸ªæ–‡ä»¶")

                # 5. æŸ¥è¯¢å¤§æ¨¡å‹
                llm_result = query_llm(final_prompt)

                # 6. ç»“æœåå¤„ç†ï¼ˆä¿å­˜ä¸ºCSVæ ¼å¼ï¼‰
                if llm_result:
                    # æå–ä»£ç å—
                    model_found_locs = extract_code_blocks(llm_result)
                    # æŒ‰æ–‡ä»¶åˆ†ç»„
                    model_found_locs_separated = extract_locs_for_files(
                        model_found_locs, file_list, keep_old_order=False
                    )
                    
                    # ä¿å­˜åŸå§‹ç»“æœåˆ°CSV
                    raw_output_csv = os.path.join(OUTPUT_DIR, "raw_outputs.csv")
                    try:
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å†™å…¥è¡¨å¤´
                        file_exists = os.path.exists(raw_output_csv)
                        with open(raw_output_csv, 'a', encoding='utf-8', newline='') as f:
                            writer = csv.writer(f)
                            if not file_exists:
                                writer.writerow(['instance_id', 'commit_hash', 'problem_statement', 'raw_llm_output', 'extracted_code_blocks'])
                            writer.writerow([instance_id, commit_hash, problem_statement, llm_result, model_found_locs])
                        print(f"åŸå§‹è¾“å‡ºä¿å­˜åˆ°CSV: {raw_output_csv}")
                    except IOError as e:
                        print(f"é”™è¯¯ï¼šæ— æ³•å†™å…¥åŸå§‹è¾“å‡ºCSVæ–‡ä»¶: {e}")
                    
                    # ä¿å­˜å¤„ç†åçš„ç»“æœåˆ°CSV
                    locations_csv = os.path.join(OUTPUT_DIR, "locations.csv")
                    try:
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å†™å…¥è¡¨å¤´
                        file_exists = os.path.exists(locations_csv)
                        with open(locations_csv, 'a', encoding='utf-8', newline='') as f:
                            writer = csv.writer(f)
                            if not file_exists:
                                writer.writerow(['instance_id', 'file_path', 'location_type', 'location_name'])
                            
                            # ä¸ºæ¯ä¸ªæ–‡ä»¶çš„æ¯ä¸ªä½ç½®å†™å…¥ä¸€è¡Œ
                            for file_path, locs in model_found_locs_separated.items():
                                for loc in locs:
                                    # è§£æä½ç½®ç±»å‹å’Œåç§°
                                    if ':' in loc:
                                        loc_type, loc_name = loc.split(':', 1)
                                        loc_type = loc_type.strip()
                                        loc_name = loc_name.strip()
                                    else:
                                        loc_type = 'unknown'
                                        loc_name = loc.strip()
                                    writer.writerow([instance_id, file_path, loc_type, loc_name])
                        print(f"ä½ç½®ä¿¡æ¯ä¿å­˜åˆ°CSV: {locations_csv}")
                    except IOError as e:
                        print(f"é”™è¯¯ï¼šæ— æ³•å†™å…¥ä½ç½®ä¿¡æ¯CSVæ–‡ä»¶: {e}")
                    
                    # ä¿å­˜è¿›åº¦ä¿¡æ¯
                    progress_file = os.path.join(OUTPUT_DIR, "progress.txt")
                    try:
                        with open(progress_file, 'w', encoding='utf-8') as f:
                            f.write(f"æœ€åå¤„ç†çš„è¡Œ: {i+1}\n")
                            f.write(f"æœ€åå¤„ç†çš„instance_id: {instance_id}\n")
                            f.write(f"å¤„ç†æ—¶é—´: {__import__('datetime').datetime.now()}\n")
                    except:
                        pass  # è¿›åº¦æ–‡ä»¶å†™å…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                        
                else:
                    print(f"æœªèƒ½ä» LLM è·å– instance_id '{instance_id}' çš„ç»“æœã€‚")

                print("-" * 50)
            
            # å¤„ç†å®Œæˆç»Ÿè®¡
            processed_count = total_rows - start_index
            print(f"\nğŸ‰ æ‰¹å¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   - èµ·å§‹è¡Œ: ç¬¬{start_index + 1}è¡Œ")
            print(f"   - ç»“æŸè¡Œ: ç¬¬{total_rows}è¡Œ")
            print(f"   - æ€»å¤„ç†: {processed_count} æ¡è®°å½•")
            print(f"   - è¾“å‡ºæ–‡ä»¶:")
            print(f"     * {raw_output_csv}")
            print(f"     * {locations_csv}")

    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ° CSV æ–‡ä»¶ {CSV_FILE_PATH}")
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


if __name__ == '__main__':
    main()
