import csv
import json
import os
import subprocess
import requests
import ast
import time
import glob
from clang.cindex import Index, CursorKind

# --- 1. é…ç½®åŒºåŸŸ ---

# è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨æœ¬åœ°çš„ Linux å†…æ ¸ä»£ç ä»“åº“è·¯å¾„
LINUX_REPO_PATH = "/root/Agentless/linux/"

# è¾“å…¥æ–‡ä»¶
CSV_FILE_PATH = 'merged_output.csv'  # æ‚¨çš„ CSV æ–‡ä»¶å
JSONL_FILE_PATH = '2.jsonl'

# å¤§æ¨¡å‹ API é…ç½® (è¯·å¡«å…¥æ‚¨çš„çœŸå®ä¿¡æ¯)
API_URL = "https://api.deepseek.com/chat/completions"
API_KEY = "" # use your own key

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "llm_results"

# æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
MAX_CONTEXT_LENGTH = 60000

# æ–‡ä»¶å†…å®¹æ¨¡æ¿ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
FILE_CONTENT_TEMPLATE = """
### File: {file_name} ###
{file_content}
"""

# --- 2. Prompt æ¨¡æ¿ ---

OBTAIN_RELEVANT_FUNCTIONS_PROMPT = """
Please analyze the following GitHub Problem Description and Structured Code Information.
The code information has been extracted using libclang and shows functions, structures, variables, and macros.

Your task is to identify the Top-10 most relevant code locations that need inspection or editing to fix the problem.
Focus on the actual functions, structures, variables, and macros that are directly related to the issue.

### GitHub Problem Description ###
{problem_statement}

### Structured Code Information ###
{file_contents}

###

Please provide the complete set of locations in this format:
- For functions: function: function_name
- For structures: struct: struct_name
- For variables: variable: variable_name
- For macros: macro: macro_name

### Examples:
    fs/ext4/inode.c
function: ext4_write_begin
function: ext4_write_end
struct: ext4_inode_info

net/core/skbuff.c
function: skb_copy_data
variable: skb_head_cache
macro: SKB_DATA_ALIGN

mm/page_alloc.c
function: alloc_pages
function: free_pages
struct: free_area
Return just the locations wrapped with ```. Focus on the most relevant items based on the problem description.
"""


# --- 3. è¾…åŠ©å‡½æ•° ---

def load_dataset_to_dict(jsonl_path):
    """ä¸€æ¬¡æ€§åŠ è½½ JSONL æ•°æ®åˆ°å­—å…¸ä¸­ï¼Œä»¥ ID ä¸ºé”®ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾ã€‚"""
    dataset = {}
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if 'id' in data:
                        dataset[data['id']] = data
                except json.JSONDecodeError:
                    print(f"è­¦å‘Šï¼šè·³è¿‡æ ¼å¼é”™è¯¯çš„ JSON è¡Œ: {line.strip()}")
        print(f"æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®ä» {jsonl_path}")
        return dataset
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {jsonl_path}")
        exit(1)

def cleanup_git_locks(repo_path):
    """æ¸…ç†Gitä»“åº“ä¸­çš„æ‰€æœ‰é”æ–‡ä»¶"""
    print(f"ğŸ§¹ å¼€å§‹æ¸…ç†Gité”æ–‡ä»¶...")
    lock_patterns = [
        '.git/index.lock', '.git/HEAD.lock', '.git/config.lock',
        '.git/refs/heads/*.lock', '.git/refs/remotes/*/*.lock', '.git/objects/pack/*.lock',
    ]
    cleaned_count = 0
    for pattern in lock_patterns:
        full_pattern = os.path.join(repo_path, pattern)
        for lock_file in glob.glob(full_pattern):
            try:
                if os.path.exists(lock_file):
                    os.remove(lock_file)
                    print(f"   âœ… åˆ é™¤: {os.path.relpath(lock_file, repo_path)}")
                    cleaned_count += 1
            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥ {lock_file}: {e}")
    if cleaned_count > 0:
        print(f"ğŸ¯ å…±æ¸…ç†äº† {cleaned_count} ä¸ªé”æ–‡ä»¶")
    else:
        print(f"â„¹ï¸  æ²¡æœ‰å‘ç°é”æ–‡ä»¶")
    return cleaned_count > 0

def checkout_commit(repo_path, commit_hash):
    """åœ¨æŒ‡å®šçš„ä»“åº“è·¯å¾„ä¸­åˆ‡æ¢åˆ°ç‰¹å®šçš„ commitã€‚"""
    if not os.path.isdir(repo_path):
        print(f"é”™è¯¯ï¼šæŒ‡å®šçš„ Linux ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}")
        return False
    print(f"æ­£åœ¨åˆ‡æ¢åˆ° commit: {commit_hash} ...")
    try:
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=repo_path, check=True, capture_output=True, text=True)
        subprocess.run(['git', 'clean', '-fd'], cwd=repo_path, check=True, capture_output=True, text=True)
        result = subprocess.run(['git', 'checkout', commit_hash], cwd=repo_path, check=True, capture_output=True, text=True)
        print("âœ… åˆ‡æ¢ commit æˆåŠŸã€‚")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ åˆ‡æ¢åˆ° commit {commit_hash} å¤±è´¥ã€‚")
        print(f"Git å‘½ä»¤è¾“å‡º:\n{e.stderr}")
        print("ğŸ”„ æ­£åœ¨å°è¯•æ¸…ç†é”æ–‡ä»¶åé‡è¯•...")
        cleanup_git_locks(repo_path)
        try:
            result = subprocess.run(['git', 'checkout', commit_hash], cwd=repo_path, check=True, capture_output=True, text=True)
            print("âœ… æ¸…ç†ååˆ‡æ¢æˆåŠŸã€‚")
            return True
        except Exception as e2:
             print(f"âŒ æ¸…ç†åé‡è¯•ä¾ç„¶å¤±è´¥: {e2}")
             return False
    except FileNotFoundError:
        print("âŒ 'git' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿ Git å·²å®‰è£…å¹¶ä½äºæ‚¨çš„ PATH ä¸­ã€‚")
        return False

def read_file_content(file_path):
    """ä½¿ç”¨libclangè§£ææ–‡ä»¶å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯"""
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return f"# æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n"
    print(f"ğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
    try:
        if file_path.endswith(('.c', '.cpp', '.cc', '.cxx', '.h', '.hpp')):
            return extract_c_structures(file_path)
        else:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                return FILE_CONTENT_TEMPLATE.format(file_name=file_path, file_content=''.join(lines))
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return f"# è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}\n"

def extract_c_structures(file_path):
    """ä½¿ç”¨libclangæå–C/C++æ–‡ä»¶çš„ç»“æ„åŒ–ä¿¡æ¯"""
    try:
        index = Index.create()
        translation_unit = index.parse(file_path, args=['-I' + os.path.join(LINUX_REPO_PATH, 'include')])

        diagnostics = list(translation_unit.diagnostics)
        if diagnostics:
            # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯ï¼Œå¿½ç•¥æç¤ºä¿¡æ¯
            severe_diags = [d for d in diagnostics if d.severity >= 3]
            if severe_diags:
                print(f"âš ï¸  libclangå‘ç° {len(severe_diags)} ä¸ªä¸¥é‡è¯Šæ–­ä¿¡æ¯ (ä»…æ˜¾ç¤ºå‰3ä¸ª):")
                for diag in severe_diags[:3]:
                    print(f"   - {diag.severity}: {diag.spelling} at {diag.location}")

        structures = [f"### File: {file_path} ###"]

        # æˆ‘ä»¬åªæå–é¡¶å±‚çš„å‡½æ•°å®šä¹‰ï¼Œä»¥ç®€åŒ–è¾“å‡º
        for node in translation_unit.cursor.get_children():
            if node.location.file and os.path.samefile(node.location.file.name, file_path):
                 if node.kind == CursorKind.FUNCTION_DECL and node.is_definition():
                    func_name = node.spelling
                    if func_name:
                        structures.append(f"function: {func_name}")

        if len(structures) <= 1:
             print(f"âš ï¸  libclangæœªèƒ½æå–ä»»ä½•å‡½æ•°ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å¼")
             with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                 return f.read()

        return '\n'.join(structures) + '\n'
    except Exception as e:
        print(f"âŒ libclangè§£æå¤±è´¥: {e}ã€‚å›é€€åˆ°æ–‡æœ¬è¯»å–æ¨¡å¼...")
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        except Exception as read_error:
            print(f"âŒ æ–‡ä»¶è¯»å–ä¹Ÿå¤±è´¥: {read_error}")
            return f"# è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}\n"


def query_llm(prompt):
    """
    å‘å¤§æ¨¡å‹å‘é€è¯·æ±‚ã€‚
    å¦‚æœæˆåŠŸï¼Œè¿”å›ç»“æœå­—ç¬¦ä¸²ã€‚
    å¦‚æœé‡åˆ°tokenè¶…é™é”™è¯¯ï¼Œè¿”å›ç‰¹æ®Šæ ‡è¯† "TOKEN_LIMIT_EXCEEDED"ã€‚
    å¦‚æœé‡åˆ°å…¶ä»–é”™è¯¯ï¼Œè¿”å› Noneã€‚
    """
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant specialized in code analysis and debugging."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 1024,
        "temperature": 0.0,
        "stream": False
    }
    print("æ­£åœ¨å‘å¤§æ¨¡å‹å‘é€è¯·æ±‚...")
    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=180)

        # æ£€æŸ¥æ˜¯å¦æ˜¯APIé”™è¯¯
        if response.status_code != 200:
            print(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            error_data = response.json()
            error_message = error_data.get("error", {}).get("message", "").lower()
            error_code = error_data.get("error", {}).get("code", "")

            # ** å…³é”®æ”¹åŠ¨ï¼šä¸“é—¨è¯†åˆ«tokenè¶…é™é”™è¯¯ **
            if "context_length_exceeded" in error_code or "token limits" in error_message:
                print("è¯†åˆ«åˆ°Tokenè¶…é™é”™è¯¯ã€‚")
                return "TOKEN_LIMIT_EXCEEDED"

            print(f"å“åº”å†…å®¹: {response.text}")
            return None

        response_data = response.json()
        if "choices" in response_data and response_data["choices"]:
            result_text = response_data["choices"][0]["message"].get("content", "")
            print("âœ… æˆåŠŸä»LLMè·å–å“åº”ã€‚")
            return result_text.strip()
        else:
            print(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {response_data}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"é”™è¯¯ï¼šAPI è¯·æ±‚å¤±è´¥: {e}")
        return None

def num_tokens_from_messages(message, model_name):
    """ä¼°ç®—tokenæ•°é‡çš„ç®€å•å®ç°"""
    return len(message) // 4

def extract_code_blocks(text):
    """ä»LLMè¾“å‡ºä¸­æå–ä»£ç å—"""
    if '```' in text:
        parts = text.split('```')
        if len(parts) >= 3:
            return parts[1].strip()
    return text.strip()

def extract_locs_for_files(model_found_locs, file_names):
    """è§£æå®šä½ç»“æœå¹¶æŒ‰æ–‡ä»¶åˆ†ç»„"""
    result = {}
    lines = model_found_locs.split('\n')
    current_file = None
    for line in lines:
        line = line.strip()
        if not line:
            continue
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶è·¯å¾„
        # æ›´ç¨³å¥çš„æ£€æŸ¥ï¼Œç¡®ä¿lineæ˜¯å·²çŸ¥çš„ç›¸å…³æ–‡ä»¶ä¹‹ä¸€
        is_file_path = False
        for fname in file_names:
            if fname in line:
                 is_file_path = True
                 current_file = fname
                 break

        if is_file_path:
             if current_file not in result:
                result[current_file] = []
        elif current_file and line.startswith(('function:', 'struct:', 'variable:', 'macro:')):
            if current_file in result:
                result[current_file].append(line)
    return result

# --- 4. ä¸»æ‰§è¡Œé€»è¾‘ ---

def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªæµç¨‹ã€‚"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    if os.path.exists(LINUX_REPO_PATH):
        cleanup_git_locks(LINUX_REPO_PATH)
    else:
        print(f"âš ï¸  Linuxä»“åº“è·¯å¾„ä¸å­˜åœ¨: {LINUX_REPO_PATH}")
        return

    dataset = load_dataset_to_dict(JSONL_FILE_PATH)
    if not dataset:
        return

    try:
        with open(CSV_FILE_PATH, mode='r', encoding='utf-8') as csvfile:
            reader = csv.reader(csvfile)
            header = next(reader)
            all_rows = list(reader)
            total_rows = len(all_rows)
            start_index = 0

            print(f"å¼€å§‹å¤„ç† CSV æ–‡ä»¶: {CSV_FILE_PATH}")
            print(f"æ€»å…± {total_rows} è¡Œæ•°æ®ï¼Œä»ç¬¬ {start_index + 1} è¡Œå¼€å§‹å¤„ç†")
            print("-" * 50)

            for i in range(start_index, total_rows):
                row = all_rows[i]
                instance_id, files_str = row
                print(f"\nå¤„ç†ç¬¬ {i+1} è¡Œ: instance_id = {instance_id} (è¿›åº¦: {i+1-start_index}/{total_rows-start_index})")

                if instance_id not in dataset:
                    print(f"è­¦å‘Šï¼šåœ¨ {JSONL_FILE_PATH} ä¸­æ‰¾ä¸åˆ° instance_id '{instance_id}'ï¼Œè·³è¿‡æ­¤è¡Œã€‚")
                    continue

                bug_data = dataset[instance_id]
                commit_hash, problem_statement = bug_data.get('commit'), bug_data.get('report')
                if not commit_hash or not problem_statement:
                    print(f"è­¦å‘Šï¼šinstance_id '{instance_id}' çš„æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡ã€‚")
                    continue

                if not checkout_commit(LINUX_REPO_PATH, commit_hash):
                    print(f"âŒ æ— æ³•åˆ‡æ¢åˆ° commitï¼Œè·³è¿‡ instance_id '{instance_id}'ã€‚")
                    continue

                try:
                    file_list = ast.literal_eval(files_str)
                except (ValueError, SyntaxError):
                    print(f"é”™è¯¯ï¼šæ— æ³•è§£ææ–‡ä»¶åˆ—è¡¨å­—ç¬¦ä¸²: {files_str}")
                    continue

                # --- é˜¶æ®µä¸€ï¼šäº‹å‰é¢„é˜²ï¼Œæ„å»ºåˆå§‹å†…å®¹åˆ—è¡¨ ---
                print("\nğŸ”„ [é˜¶æ®µ1] æ­£åœ¨é¢„å…ˆè¯»å–å¹¶ç­›é€‰æ–‡ä»¶ï¼Œé¿å…æ˜æ˜¾è¶…é•¿...")
                content_blocks = []
                files_for_prompt = []
                for rel_path in file_list:
                    full_path = os.path.join(LINUX_REPO_PATH, rel_path)
                    file_content = read_file_content(full_path)

                    # æ„é€ ä¸´æ—¶promptè¿›è¡Œé¢„æ£€æŸ¥
                    temp_agg_content = "\n\n".join(content_blocks + [file_content])
                    temp_prompt = OBTAIN_RELEVANT_FUNCTIONS_PROMPT.format(problem_statement=problem_statement, file_contents=temp_agg_content)

                    if num_tokens_from_messages(temp_prompt, "deepseek-chat") >= MAX_CONTEXT_LENGTH:
                        print(f"âš ï¸  é¢„æ£€æŸ¥å‘ç°æ·»åŠ æ–‡ä»¶ {rel_path} åå¯èƒ½è¶…é•¿ï¼Œåœæ­¢æ·»åŠ ã€‚")
                        break

                    content_blocks.append(file_content)
                    files_for_prompt.append(rel_path)
                    print(f"   âœ… é¢„æ·»åŠ æ–‡ä»¶: {rel_path}")

                if not content_blocks:
                    print(f"âŒ å³ä½¿æ˜¯ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¹Ÿå¯èƒ½è¶…é•¿ï¼Œæˆ–æ‰€æœ‰æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè·³è¿‡æ­¤å®ä¾‹ã€‚")
                    continue

                # --- é˜¶æ®µäºŒï¼šäº‹åé‡è¯•ï¼Œå¾ªç¯è°ƒç”¨LLM ---
                print(f"\nğŸ”„ [é˜¶æ®µ2] å¼€å§‹è¯·æ±‚LLMï¼ŒåŒ…å«å¤±è´¥é‡è¯•æœºåˆ¶...")
                max_retries = len(content_blocks)
                llm_result = None

                for attempt in range(max_retries):
                    print(f"   å°è¯• #{attempt + 1}/{max_retries}ï¼Œä½¿ç”¨ {len(content_blocks)} ä¸ªæ–‡ä»¶...")

                    # æ„å»ºå½“å‰å°è¯•çš„prompt
                    current_contents_agg = "\n\n".join(content_blocks)
                    final_prompt = OBTAIN_RELEVANT_FUNCTIONS_PROMPT.format(
                        problem_statement=problem_statement,
                        file_contents=current_contents_agg
                    )

                    # è°ƒç”¨LLM
                    llm_result = query_llm(final_prompt)

                    # åˆ†æç»“æœ
                    if llm_result == "TOKEN_LIMIT_EXCEEDED":
                        if len(content_blocks) > 1:
                            removed_file = files_for_prompt.pop()
                            content_blocks.pop()
                            print(f"   - APIè¿”å›Tokenè¶…é™ï¼Œç§»é™¤æœ€åä¸€ä¸ªæ–‡ä»¶ ({os.path.basename(removed_file)}) åé‡è¯•...")
                            time.sleep(1) # çŸ­æš‚ç­‰å¾…
                        else:
                            print("   - å³ä½¿åªç”¨ä¸€ä¸ªæ–‡ä»¶ä¹Ÿè¶…é™ï¼Œæ— æ³•å†ç¼©å‡ã€‚æ”¾å¼ƒã€‚")
                            llm_result = None # æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
                            break
                    else:
                        # æˆåŠŸæˆ–é‡åˆ°å…¶ä»–ä¸å¯é‡è¯•çš„é”™è¯¯
                        break

                # --- é˜¶æ®µä¸‰ï¼šå¤„ç†å¹¶ä¿å­˜ç»“æœ ---
                print("\nğŸ”„ [é˜¶æ®µ3] å¤„ç†æœ€ç»ˆç»“æœ...")
                if llm_result:
                    model_found_locs = extract_code_blocks(llm_result)
                    model_found_locs_separated = extract_locs_for_files(model_found_locs, files_for_prompt)

                    # ä¿å­˜åŸå§‹ç»“æœ
                    raw_output_csv = os.path.join(OUTPUT_DIR, "raw_outputs.csv")
                    file_exists = os.path.exists(raw_output_csv)
                    with open(raw_output_csv, 'a', encoding='utf-8', newline='') as f:
                        writer = csv.writer(f)
                        if not file_exists:
                            writer.writerow(['instance_id', 'commit_hash', 'problem_statement', 'raw_llm_output', 'extracted_code_blocks'])
                        writer.writerow([instance_id, commit_hash, problem_statement, llm_result, model_found_locs])
                    print(f"   - åŸå§‹è¾“å‡ºå·²ä¿å­˜åˆ°: {raw_output_csv}")

                    # ä¿å­˜å¤„ç†åçš„ä½ç½®ä¿¡æ¯
                    locations_csv = os.path.join(OUTPUT_DIR, "locations.csv")
                    file_exists = os.path.exists(locations_csv)
                    with open(locations_csv, 'a', encoding='utf-8', newline='') as f:
                        writer = csv.writer(f)
                        if not file_exists:
                            writer.writerow(['instance_id', 'file_path', 'location_type', 'location_name'])
                        for file_path, locs in model_found_locs_separated.items():
                            for loc in locs:
                                if ':' in loc:
                                    loc_type, loc_name = loc.split(':', 1)
                                    writer.writerow([instance_id, file_path, loc_type.strip(), loc_name.strip()])
                                else:
                                    writer.writerow([instance_id, file_path, 'unknown', loc.strip()])
                    print(f"   - ä½ç½®ä¿¡æ¯å·²ä¿å­˜åˆ°: {locations_csv}")

                else:
                    print(f"   - æœªèƒ½ä» LLM è·å– instance_id '{instance_id}' çš„æœ‰æ•ˆç»“æœã€‚")

                print("-" * 50)

            print("\nğŸ‰ æ‰¹å¤„ç†å®Œæˆ!")

    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ° CSV æ–‡ä»¶ {CSV_FILE_PATH}")
    except Exception as e:
        import traceback
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        traceback.print_exc()


if __name__ == '__main__':
    main()
