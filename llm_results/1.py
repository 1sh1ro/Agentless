#!/usr/bin/env python3
"""
åˆå¹¶locations.csvæ–‡ä»¶çš„è„šæœ¬
- å°†ç›¸åŒinstance_idçš„è®°å½•åˆå¹¶
- å°†ç›¸åŒæ–‡ä»¶è·¯å¾„çš„è®°å½•åˆå¹¶
- ç”Ÿæˆæ›´æ¸…æ™°çš„è¾“å‡ºæ–‡ä»¶
"""

import csv
import os
from collections import defaultdict
import json

def load_locations_csv(file_path):
    """åŠ è½½locations.csvæ–‡ä»¶"""
    print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)
    
    print(f"âœ… æˆåŠŸè¯»å– {len(data)} æ¡è®°å½•")
    return data

def merge_by_instance_and_file(data):
    """æŒ‰instance_idå’Œfile_pathåˆ†ç»„åˆå¹¶"""
    print(f"ğŸ”„ å¼€å§‹æŒ‰instance_idå’Œfile_pathåˆ†ç»„...")
    
    # ä½¿ç”¨åµŒå¥—å­—å…¸: instance_id -> file_path -> locations
    grouped = defaultdict(lambda: defaultdict(list))
    
    for row in data:
        instance_id = row['instance_id']
        file_path = row['file_path']
        location_info = {
            'type': row['location_type'],
            'name': row['location_name']
        }
        grouped[instance_id][file_path].append(location_info)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_instances = len(grouped)
    total_files = sum(len(files) for files in grouped.values())
    total_locations = sum(
        sum(len(locations) for locations in files.values()) 
        for files in grouped.values()
    )
    
    print(f"ğŸ“Š åˆ†ç»„ç»Ÿè®¡:")
    print(f"   - å”¯ä¸€instance_id: {total_instances} ä¸ª")
    print(f"   - å”¯ä¸€æ–‡ä»¶: {total_files} ä¸ª")
    print(f"   - æ€»ä½ç½®æ•°: {total_locations} ä¸ª")
    
    return grouped

def save_merged_csv(grouped_data, output_file):
    """ä¿å­˜åˆå¹¶åçš„CSVæ–‡ä»¶"""
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        
        # å†™å…¥è¡¨å¤´
        writer.writerow(['instance_id', 'file_path', 'location_count', 'functions', 'structs', 'variables', 'macros', 'others'])
        
        for instance_id, files in grouped_data.items():
            for file_path, locations in files.items():
                # æŒ‰ç±»å‹åˆ†ç»„ä½ç½®
                functions = []
                structs = []
                variables = []
                macros = []
                others = []
                
                for loc in locations:
                    loc_type = loc['type'].lower()
                    loc_name = loc['name']
                    
                    if 'function' in loc_type:
                        functions.append(loc_name)
                    elif 'struct' in loc_type:
                        structs.append(loc_name)
                    elif 'variable' in loc_type or 'var' in loc_type:
                        variables.append(loc_name)
                    elif 'macro' in loc_type:
                        macros.append(loc_name)
                    else:
                        others.append(f"{loc_type}:{loc_name}")
                
                # å»é‡å¹¶æ’åº
                functions = sorted(list(set(functions)))
                structs = sorted(list(set(structs)))
                variables = sorted(list(set(variables)))
                macros = sorted(list(set(macros)))
                others = sorted(list(set(others)))
                
                total_count = len(functions) + len(structs) + len(variables) + len(macros) + len(others)
                
                # å†™å…¥è¡Œæ•°æ®
                writer.writerow([
                    instance_id,
                    file_path,
                    total_count,
                    '; '.join(functions) if functions else '',
                    '; '.join(structs) if structs else '',
                    '; '.join(variables) if variables else '',
                    '; '.join(macros) if macros else '',
                    '; '.join(others) if others else ''
                ])
    
    print(f"âœ… åˆå¹¶åçš„CSVæ–‡ä»¶å·²ä¿å­˜")

def save_summary_json(grouped_data, output_file):
    """ä¿å­˜è¯¦ç»†çš„JSONæ ¼å¼æ‘˜è¦"""
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜JSONæ‘˜è¦åˆ°: {output_file}")
    
    summary = {}
    
    for instance_id, files in grouped_data.items():
        instance_summary = {
            'total_files': len(files),
            'files': {}
        }
        
        for file_path, locations in files.items():
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = defaultdict(int)
            type_details = defaultdict(list)
            
            for loc in locations:
                loc_type = loc['type']
                loc_name = loc['name']
                type_counts[loc_type] += 1
                type_details[loc_type].append(loc_name)
            
            # å»é‡
            for loc_type in type_details:
                type_details[loc_type] = sorted(list(set(type_details[loc_type])))
            
            instance_summary['files'][file_path] = {
                'total_locations': len(locations),
                'type_counts': dict(type_counts),
                'locations': dict(type_details)
            }
        
        summary[instance_id] = instance_summary
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… JSONæ‘˜è¦æ–‡ä»¶å·²ä¿å­˜")

def generate_statistics_report(grouped_data, output_file):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_instances = len(grouped_data)
    all_files = set()
    all_location_types = defaultdict(int)
    instances_by_file_count = defaultdict(int)
    files_by_location_count = defaultdict(int)
    
    for instance_id, files in grouped_data.items():
        instances_by_file_count[len(files)] += 1
        
        for file_path, locations in files.items():
            all_files.add(file_path)
            files_by_location_count[len(locations)] += 1
            
            for loc in locations:
                all_location_types[loc['type']] += 1
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ä½ç½®æ•°æ®ç»Ÿè®¡æŠ¥å‘Š\n\n")
        
        f.write(f"## æ€»ä½“ç»Ÿè®¡\n")
        f.write(f"- æ€»instanceæ•°é‡: {total_instances}\n")
        f.write(f"- å”¯ä¸€æ–‡ä»¶æ•°é‡: {len(all_files)}\n")
        f.write(f"- æ€»ä½ç½®æ•°é‡: {sum(all_location_types.values())}\n\n")
        
        f.write(f"## æŒ‰ä½ç½®ç±»å‹ç»Ÿè®¡\n")
        for loc_type, count in sorted(all_location_types.items(), key=lambda x: x[1], reverse=True):
            f.write(f"- {loc_type}: {count} ä¸ª\n")
        f.write("\n")
        
        f.write(f"## æŒ‰æ–‡ä»¶æ•°é‡åˆ†å¸ƒ\n")
        for file_count, instance_count in sorted(instances_by_file_count.items()):
            f.write(f"- {file_count} ä¸ªæ–‡ä»¶çš„instance: {instance_count} ä¸ª\n")
        f.write("\n")
        
        f.write(f"## æ–‡ä»¶ä¸­ä½ç½®æ•°é‡åˆ†å¸ƒ\n")
        for loc_count, file_count in sorted(files_by_location_count.items()):
            f.write(f"- {loc_count} ä¸ªä½ç½®çš„æ–‡ä»¶: {file_count} ä¸ª\n")
        f.write("\n")
        
        f.write(f"## æœ€å¸¸å‡ºç°çš„æ–‡ä»¶ (Top 10)\n")
        file_frequency = defaultdict(int)
        for files in grouped_data.values():
            for file_path in files.keys():
                file_frequency[file_path] += 1
        
        top_files = sorted(file_frequency.items(), key=lambda x: x[1], reverse=True)[:10]
        for file_path, frequency in top_files:
            f.write(f"- {file_path}: å‡ºç°åœ¨ {frequency} ä¸ªinstanceä¸­\n")
    
    print(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆå¹¶locations.csvæ–‡ä»¶")
    print("=" * 50)
    
    # è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_file = "locations.csv"
    output_csv = "final.csv"
    output_json = "locations_summary.json"
    output_report = "locations_statistics.md"
    
    # 1. åŠ è½½æ•°æ®
    data = load_locations_csv(input_file)
    if data is None:
        return
    
    # 2. åˆ†ç»„åˆå¹¶
    grouped_data = merge_by_instance_and_file(data)
    
    # 3. ä¿å­˜åˆå¹¶åçš„CSV
    save_merged_csv(grouped_data, output_csv)
    
    # 4. ä¿å­˜JSONæ‘˜è¦
    save_summary_json(grouped_data, output_json)
    
    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_statistics_report(grouped_data, output_report)
    
    print("\nğŸ‰ å¤„ç†å®Œæˆ!")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   ğŸ“Š åˆå¹¶åçš„CSV: {output_csv}")
    print(f"   ğŸ“‹ è¯¦ç»†JSONæ‘˜è¦: {output_json}")
    print(f"   ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Š: {output_report}")

if __name__ == "__main__":
    main()
